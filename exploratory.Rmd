---
title: "Exploratory data analysis"
output: html_notebook
---

This is an initial exploratory analysis before building any models.

# Loading libraries
```{r}
source("data/feature_engineering.R")
```
# Loading current data

```{r}
train_data=fread('data/train.csv')
head(train_data)
```
# Summary
```{r}
summary(train_data)
```

Our baseline for survival rate is 0.38, that is the "no information" chance of survival. 
Lets take a look at the what seems to be the categorical variables:

```{r}
categorical=colnames(train_data)[c(3,4,5,8,11,12)]
rbindlist(lapply(categorical,function(s) data.table(category=s,len=dim(train_data[,.N, by=s])[1])))
```

It looks we need the "name", "sex" and "cabin" features in order to extract any meaninful information from it.

## Cleaning Name feature
What feature in the name may be an indicator for survival? Let's take a look
```{r}
train_data[,head(name)]
```
Our first hypothesis: either alias (a well known subject) or its title is an indication for greater survival rate.
Lets extract those and try to correlate with the survival rate.

### Defining new features from name
```{r}
train_data[,title:=sub("\\..*","", sub(".*, ","",name))]
train_data[,has_nick:=grepl("(\\(|\\\")",name,fixed=F)]
train_data[,.(name,title,has_nick)]
```

### Correlation with survival
```{r}
train_data[,title:=sub("\\..*","", sub(".*, ","",name))]
train_data[,has_nick:=grepl("(\\(|\\\")",name,fixed=F)]
train_data[,.(.N,mean(survived)), title]
```
The top 6 titles, may constitute our base category, we leaver the remaining as the "other" category
```{r} 
train_data[,.(.N,mean(survived)), has_nick]
```
Its seem our assumptions were correct, those with nickname or a distinctive title have a greater survival rate than those who don't.

## Cleaning the sex variable

```{r}
train_data[,.N,sex]
```

```{r}
train_data[,.(.N,rate=mean(survived)),.(is_male=grepl("^m",sex,ignore.case = T),is_female=grepl("^(w|f)",sex,ignore.case = T),is_sex_unknown=sex=="")]
```
The unknown sex could be inferred from the title, but not for all titles:
```{r}
train_data[sex!="",mean(is_female=grepl("^(w|f)",sex,ignore.case = T)),.(title)]
```
## Cabin variable
```{r}
train_data[,.(.N,mean(survived)),cabin==""]
```
## Embarked variable
```{r}
train_data[,.(.N,mean(survived)),embarked]
```

## Pclass variable
```{r}
train_data[,.(.N,mean(survived)),pclass]
```
## Parch variable
```{r}
train_data[,.(.N,mean(survived)),parch]
```

## ticket variable
```{r}
train_data[,head(ticket)]
```

## ticket prefix
```{r}
extract_category=function(ticket){
  has_category=grepl(" ",ticket)
  if(has_category){
    ret=strsplit(ticket,split=" ")[[1]]
    data.table(category=ret[1],value=as.integer(ret[2]))
  }
  else{
    if (is.na(as.integer(ticket))){
      data.table(category=ticket,value=0)  
    }else{
      data.table(category="NA",value=(ticket))
    }
  }
}
ticket=cbind(rbindlist(lapply(train_data$ticket,extract_category)),train_data[,.(ticket,survived=survived)])
ticket[,.(.N,rate=mean(survived),mean(as.integer(value))),.(category)][order(-rate)]
```

### survival rate by ticket order number and prefix
```{r}
ticket=ticket[order(value)]
ticket$x=1:dim(ticket)[1]
ggplot(ticket[!is.na(as.integer(value))],aes(x=x,y=survived,color=category=="NA"))+geom_smooth()
```

#  Moving average of survival rate by ticket number order for tickets without a prefix
```{r}
x=20:dim(ticket[category=="NA"])[1]
y= sapply(x,function(i) ticket[category=="NA"][(i-19:i)][,mean(survived)])
plot(x,y)
```

#  Moving average of survival rate by ticket number order for tickets with a prefix
```{r}
x=20:dim(ticket[category!="NA"])[1]
y= sapply(x,function(i) ticket[category!="NA"][(i-19:i)][,mean(survived)])
plot(x,y)
```
#  Moving average of survival rate by age order 

```{r}
train_data=train_data[order(age)]
x=20:dim(train_data[!is.na(age)])[1]
y= sapply(x,function(i) train_data[!is.na(age)][(i-19:i)][,mean(survived)])
plot(x,y)
```

#  Exploratory model
```{r}
train_data=fread('data/train.csv')
data=copy(train_data)
suppressWarnings({transformed=addfeatures(data,train_data )})
summary(transformed$data)
```

```{r}
# Remove constant columns then perform PCA to remove collinearity
set.seed(123)
input.no_preproces=cbind(transformed$target,transformed$data)
train_ind <- sample(seq_len(nrow(input.no_preproces)), size = as.integer(0.7*nrow(input.no_preproces)))
input.train=input.no_preproces[train_ind]
input.validate=input.no_preproces[-train_ind]

data.pca=prcomp(input.train[,-"target"])
#dim(data.pca$x)
pairs(data.pca$x[,1:8])
```

```{r}
regressors=data.table(data.pca$x[,c(1:25)])
regressors$target=input.train$target
data.lda = lda(target ~ ., regressors)
plot(data.lda)
```

```{r}
pred.train=predict(data.lda)

confusionMatrix(pred.train$class,as.factor(regressors$target),positive="1")
```
```{r}
input.pca=predict(data.pca,input.train[,-"target"])
lda.y=predict(data.lda,data.table(input.pca[,c(1:25)]))
acc_train=mean(input.train$target==lda.y$class)
input.pca.validate=predict(data.pca,input.validate[,-"target"])
lda.y.validate=predict(data.lda,data.table(input.pca.validate[,c(1:25)]))
acc_validate=mean(input.validate$target==lda.y.validate$class)
c(acc_train,acc_validate)
```

Let's build a random forest model with cross-validation and perform an extensive gridsearch.

```{r}
# train model
control <- trainControl(method="repeatedcv", number=11, repeats=9)
tunegrid <- expand.grid(.mtry=c(seq(1,10,3)), .ntree=c(100,400,900))
metric <- "Accuracy"
seed=814
set.seed(seed)
registerDoMC(cores=7)
custom <- train(factor(target) ~., data=input, method=customRF, metric=metric, tuneGrid=tunegrid, trControl=control)
ggplot(custom$results,aes(x=mtry,y=ntree,
                          size=-1/log(Accuracy),
                          color=-1/log(Accuracy/max(Accuracy+AccuracySD))))+geom_point()+geom_point(aes(size=-1/log(Accuracy+AccuracySD)),shape=1)
```
```{r}
plot_grid_search(mtry_grid = seq(10,100,1)/10,ntree_grid = seq(100,900,10), results=custom$results)
```

```{r}
# train model
control <- trainControl(method="repeatedcv", number=11, repeats=9)
#tunegrid <- expand.grid(.mtry=c(seq(1,15,3)), .ntree=c(100, 400,900, 1600, 2500))
tunegrid <- expand.grid(.mtry=c(seq(3,8,1)), .ntree=c(seq(100,700,100)))
metric <- "Accuracy"
seed=814
set.seed(seed)
registerDoMC(cores=7)
custom.zoom <- train(factor(target) ~., data=input, method=customRF, metric=metric, tuneGrid=tunegrid, trControl=control)
#summary(custom)
ggplot(custom.zoom$results,aes(x=mtry,y=ntree,
                          size=-1/log(Accuracy),
                          color=-1/log(Accuracy/max(Accuracy+AccuracySD))))+geom_point()+geom_point(aes(size=-1/log(Accuracy+AccuracySD)),shape=1)
```

```{r}
plot_grid_search(mtry_grid = seq(20,90,1)/10,ntree_grid = seq(100,700,10), results=custom.zoom$results)
```

```{r}
results=data.table(custom.zoom$results)
results[mtry == custom.zoom$bestTune$mtry][ntree==custom.zoom$bestTune$ntree]
```

What about RF without pre-processing?
```{r}
control <- trainControl(method="repeatedcv", number=11, repeats=9)
#tunegrid <- expand.grid(.mtry=c(seq(1,15,3)), .ntree=c(100, 400,900, 1600, 2500))
tunegrid <- expand.grid(.mtry=c(seq(1,20,3)), .ntree=c(50,100,400,900,1600))
metric <- "Accuracy"
seed=814
set.seed(seed)
registerDoMC(cores=7)
custom.no_preprocess <- train(factor(target) ~., data=input.no_preproces, method=customRF, metric=metric, tuneGrid=tunegrid, trControl=control)
#summary(custom)
ggplot(custom.no_preprocess$results,aes(x=mtry,y=ntree,
                          size=-1/log(Accuracy),
                          color=-1/log(Accuracy/max(Accuracy+AccuracySD))))+geom_point()+geom_point(aes(size=-1/log(Accuracy+AccuracySD)),shape=1)
```
```{r}
plot_grid_search(mtry_grid = seq(10,210,1)/10,ntree_grid = seq(100,1600,50), results=custom.no_preprocess$results)
```


```{r}
results=data.table(custom.no_preprocess$results)
results[mtry == custom.no_preprocess$bestTune$mtry][ntree==custom.no_preprocess$bestTune$ntree]
```

```{r}
control <- trainControl(method="repeatedcv", number=11, repeats=9)
tunegrid <- expand.grid(.mtry=c(seq(4,15,1)), .ntree=c(100,200,250,300,350,400,450,500))
metric <- "Accuracy"
seed=814
set.seed(seed)
registerDoMC(cores=7)
input.no_preproces=cbind(target,transformed$data)
custom.no_preprocess_pan_grid <- train(factor(target) ~., data=input.no_preproces, method=customRF, metric=metric, tuneGrid=tunegrid, trControl=control)
#summary(custom)
ggplot(custom.no_preprocess_pan_grid$results,aes(x=mtry,y=ntree,
                          size=-1/log(Accuracy),
                          color=-1/log(Accuracy/max(Accuracy+AccuracySD))))+geom_point()+geom_point(aes(size=-1/log(Accuracy+AccuracySD)),shape=1)
```
```{r}
plot_grid_search(mtry_grid = seq(30,160,1)/10,ntree_grid = seq(50,550,10), results=custom.no_preprocess_pan_grid$results)
```

```{r}
results=data.table(custom.no_preprocess_pan_grid$results)
#results[mtry >= 7 & mtry < 13]
results[mtry == custom.no_preprocess_pan_grid$bestTune$mtry][ntree==custom.no_preprocess_pan_grid$bestTune$ntree]
```

```{r}
control <- trainControl(method="repeatedcv", number=11, repeats=9)
#tunegrid <- expand.grid(.mtry=c(seq(1,15,3)), .ntree=c(100, 400,900, 1600, 2500))
tunegrid <- expand.grid(.mtry=c(seq(7,11,1)), .ntree=c(300,325,350,375,400,425,450))
metric <- "Accuracy"
seed=814
set.seed(seed)
registerDoMC(cores=7)
input.no_preproces=cbind(target,transformed$data)
custom.no_preprocess_zoom <- train(factor(target) ~., data=input.no_preproces, method=customRF, metric=metric, tuneGrid=tunegrid, trControl=control)
#summary(custom)
ggplot(custom.no_preprocess_zoom$results,aes(x=mtry,y=ntree,
                          size=-1/log(Accuracy),
                          color=-1/log(Accuracy/max(Accuracy+AccuracySD))))+geom_point()+geom_point(aes(size=-1/log(Accuracy+AccuracySD)),shape=1)
```
```{r}
plot_grid_search(mtry_grid = seq(70,120,1)/10,ntree_grid = seq(300,500,10), results=custom.no_preprocess_zoom$results)
```

```{r}
results=data.table(custom.no_preprocess_zoom$results)
results[mtry == custom.no_preprocess_zoom$bestTune$mtry][ntree==custom.no_preprocess_zoom$bestTune$ntree]
```
```{r}
control <- trainControl(method="repeatedcv", number=11, repeats=9)
#tunegrid <- expand.grid(.mtry=c(seq(1,15,3)), .ntree=c(100, 400,900, 1600, 2500))
tunegrid <- expand.grid(.mtry=c(seq(9,12,1)), .ntree=c(200,250,275,300,325,350,375,400))
metric <- "Accuracy"
seed=814
set.seed(seed)
registerDoMC(cores=7)
input.no_preproces=cbind(target,transformed$data)
custom.no_preprocess_zoom2 <- train(factor(target) ~., data=input.no_preproces, method=customRF, metric=metric, tuneGrid=tunegrid, trControl=control)
#summary(custom)
all=rbind(custom.no_preprocess$results,
  custom.no_preprocess_pan_grid$results,
          custom.no_preprocess_zoom$results,
             custom.no_preprocess_zoom2$results)
ggplot(all,aes(x=mtry,y=ntree,
                          size=-1/log(Accuracy),
                          color=-1/log(Accuracy/max(Accuracy+AccuracySD))))+geom_point()+geom_point(aes(size=-1/log(Accuracy+AccuracySD)),shape=1)
```
```{r}
plot_grid_search(mtry_grid = seq(50,150,1)/10,ntree_grid = seq(100,500,5), results=all,corte=30,degree=1)
```

```{r}
data.table(all)[which.max(Accuracy)]
```

#Train model at the exact sweet spot
```{r}
tunegrid=data.table(mtry=11,ntree=250)
#control <- trainControl(method="repeatedcv", number=11, repeats=3)

custom.final <- train(factor(target) ~., data=input.train, method=customRF, metric=metric, tuneGrid=tunegrid)

c(input.train[,mean(target)],
input.validate[,mean(target)])
```


```{r}
ROC=data.table(predict(custom.final,input.train,type="prob"))[,c(2)]
colnames(ROC)="probability"
ROC$reference=input.train$target
ROC_curve=rbindlist(lapply(seq(1,100)/100,function(tr) data.table(tr=tr,ACC=ROC[,mean((probability > tr) == (reference == 1)) ])))
ggplot(ROC_curve,aes(x=tr,y=ACC))+geom_line()+xlab("threshold")+ylab("Accuracy")
```
```{r}
min_tr=ROC_curve[which.max(ACC),.(tr,ACC)]
max_tr=ROC_curve[order(-tr)][which.max(ACC),.(tr,ACC)]
rbind(min_tr,max_tr)
```


Let's compare with the lda ROC
```{r}
ROC=data.table(lda.y$posterior)[,c(2)]
colnames(ROC)="probability"
ROC$reference=input.train$target
ROC_curve=rbindlist(lapply(seq(1,100)/100,function(tr) data.table(tr=tr,ACC=ROC[,mean((probability > tr) == (reference == 1)) ])))
ggplot(ROC_curve,aes(x=tr,y=ACC))+geom_line()+xlab("threshold")+ylab("Accuracy")
```


Since we have a limited seats on the boat we can get the cutoff probability from the number of possible people that can be saved.
```{r}
y=predict(custom.final,input.train,type="prob")[2]
tdata=copy(input.train)
tdata$y=y
tdata=tdata[order(-y)]
tdata$i=1:dim(tdata)[1]
tdata$survival_rate=cumsum(tdata$target)/tdata$i
tdata[1:sum(target),.(rate=mean(target),total_survivors=sum(target))]
```

Graphically 
```{r}
ggplot(tdata,aes(x=i,y=survival_rate))+geom_line()+geom_vline(xintercept =sum(tdata$target))
```
The vertical line is the x=`{r} sum(tdata$target)` number of possible survivors.

```{r}
tdata[,table(y>0.5,target)]
```

```{r}
y=predict(custom.final,input.validate,type="prob")[2]
tdata=copy(input.validate)
tdata$y=y
tdata=tdata[order(-y)]
tdata$i=1:dim(tdata)[1]
tdata$survival_rate=cumsum(tdata$target)/tdata$i
tdata[1:sum(target),.(rate=mean(target),total_survivors=sum(target))]
#ggplot(tdata,aes(x=i,y=survival_rate))+geom_line()
confusionMatrix(table(tdata$y>0.5,tdata$target=="1")
```
Even though we have conducted a cross-validation the model seems to be overfitted

For now the lda model seem more reliable than the RF.

```{r}
y=data.table(lda.y$posterior)[,c(2)]
tdata=copy(input.train)
tdata$y=y
tdata=tdata[order(-y)]
tdata$i=1:dim(tdata)[1]
tdata$survival_rate=cumsum(tdata$target)/tdata$i
tdata[1:sum(target),.(rate=mean(target),total_survivors=sum(target),y=min(y))]
ggplot(tdata,aes(x=i,y=survival_rate))+geom_line()+geom_vline(xintercept =sum(tdata$target))
```

Before a recomendation is given adicional investigation must be performed.
